{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import time\n",
    "import random\n",
    "from unittest.mock import create_autospec\n",
    "import warnings\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.optim import lr_scheduler, AdamW\n",
    "\n",
    "from dataset import * #create_boolq_dataset_object, create_dataset_object, load_agnews_dataset, load_boolq_dataset, load_cb_dataset, load_imdb_dataset, load_topic_dataset, load_yelp_dataset\n",
    "from dataloader import get_dataloaders\n",
    "\n",
    "from train import test_model, train_model\n",
    "\n",
    "from prompt import PROMPTEmbedding\n",
    "from model import APT\n",
    "from utils import freeze_params_encoder, get_accuracy, count_parameters, freeze_params\n",
    "\n",
    "from transformers import RobertaTokenizer, BertTokenizer, BertForSequenceClassification, RobertaForSequenceClassification\n",
    "from transformers import get_linear_schedule_with_warmup, logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'imdb'   #imdb\n",
    "\n",
    "model_type = 'bert-base-cased'   #roberta\n",
    "\n",
    "number_of_tokens = 20\n",
    "\n",
    "mode = 'head'\n",
    "\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "learning_rate = 2e-5\n",
    "\n",
    "epochs = 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB dataset loaded succesfully\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(model_type)\n",
    "\n",
    "#tokenizer = RobertaTokenizer.from_pretrained(model_type)\n",
    "\n",
    "train_text, train_labels, test_text, test_labels, valid_text, valid_labels = load_imdb_dataset(dataset)\n",
    "\n",
    "train_data_object = create_dataset_object(train_text, train_labels, number_of_tokens, tokenizer, dataset,  mode)\n",
    "\n",
    "test_data_object  = create_dataset_object(test_text, test_labels, number_of_tokens, tokenizer, dataset, mode)\n",
    "\n",
    "val_data_object = create_dataset_object(valid_text, valid_labels, number_of_tokens, tokenizer, dataset, mode)\n",
    "\n",
    "dataloaders = get_dataloaders(train_data_object, test_data_object, val_data_object, batch_size)\n",
    "\n",
    "num_labels = 2\n",
    "\n",
    "print(\"IMDB dataset loaded succesfully\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'model = RobertaForSequenceClassification.from_pretrained(model_type, \\n                                                    num_labels=num_labels,\\n                                                    output_attentions=False,\\n                                                    output_hidden_states=False)\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(model_type, \n",
    "                                                    num_labels=num_labels,\n",
    "                                                    output_attentions=False,\n",
    "                                                    output_hidden_states=False)\n",
    "\n",
    "\n",
    "\"\"\"model = RobertaForSequenceClassification.from_pretrained(model_type, \n",
    "                                                    num_labels=num_labels,\n",
    "                                                    output_attentions=False,\n",
    "                                                    output_hidden_states=False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model for head finetuning loaded successfully\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model =  freeze_params_encoder(model, model_type)\n",
    "\n",
    "print(\"Model for head finetuning loaded successfully\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+------------+\n",
      "|         Modules          | Parameters |\n",
      "+--------------------------+------------+\n",
      "| bert.pooler.dense.weight |   589824   |\n",
      "|  bert.pooler.dense.bias  |    768     |\n",
      "+--------------------------+------------+\n",
      "Total Trainable Params: 590592\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "\tif 'classifier' not in name: # classifier layer\n",
    "\t\tif \"pooler\" not in name:\n",
    "\t\t\tparam.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roberta APT model loaded successfully\n",
      "\n"
     ]
    }
   ],
   "source": [
    "roberta = freeze_params(model)\n",
    "\n",
    "prompt_emb = PROMPTEmbedding(roberta.get_input_embeddings(), \n",
    "            n_tokens= number_of_tokens, \n",
    "            initialize_from_vocab=True)\n",
    "\n",
    "roberta.set_input_embeddings(prompt_emb)\n",
    "\n",
    "model = APT(roberta, 8, 1, model_type)\n",
    "\n",
    "print(\"Roberta APT model loaded successfully\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bert APT model loaded successfully\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bert = freeze_params(model)\n",
    "\n",
    "prompt_emb = PROMPTEmbedding(bert.get_input_embeddings(), \n",
    "            n_tokens= number_of_tokens, \n",
    "            initialize_from_vocab=True)\n",
    "\n",
    "bert.set_input_embeddings(prompt_emb)\n",
    "\n",
    "model = APT(bert, 8, 1, model_type)\n",
    "\n",
    "print(\"Bert APT model loaded successfully\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#Loss function\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr = learning_rate, eps=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=len(dataloaders['Train'])*epochs/15, \n",
    "    num_training_steps=len(dataloaders['Train'])*epochs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 512, 2])\n"
     ]
    }
   ],
   "source": [
    "phase = 'Train'\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "for idx, (data, labels) in enumerate(dataloaders[phase]):\n",
    "    input_ids =  data['input_ids'].squeeze(1).to(device)\n",
    "    attention_mask = data['attention_mask'].squeeze(1).to(device)\n",
    "    \n",
    "    \n",
    "    labels = labels.to(device)\n",
    "\n",
    "\n",
    "    output = model(input_ids = input_ids, attention_mask = attention_mask)\n",
    "\n",
    "\n",
    "    print(output.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0043, -0.4028],\n",
       "         [-0.0344, -0.2357],\n",
       "         [-0.0636, -0.2666],\n",
       "         ...,\n",
       "         [ 0.0172, -0.3997],\n",
       "         [ 0.0254, -0.2515],\n",
       "         [ 0.0690, -0.1504]],\n",
       "\n",
       "        [[ 0.0179, -0.4897],\n",
       "         [-0.1077, -0.2106],\n",
       "         [-0.1459, -0.2359],\n",
       "         ...,\n",
       "         [-0.1453, -0.3558],\n",
       "         [-0.0319, -0.2295],\n",
       "         [-0.0077, -0.1383]],\n",
       "\n",
       "        [[-0.0101, -0.4599],\n",
       "         [-0.1921, -0.2222],\n",
       "         [-0.1778, -0.1626],\n",
       "         ...,\n",
       "         [-0.1719, -0.3139],\n",
       "         [-0.0798, -0.1796],\n",
       "         [-0.0667, -0.0880]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.0723, -0.3978],\n",
       "         [-0.0255, -0.1854],\n",
       "         [-0.1084, -0.2690],\n",
       "         ...,\n",
       "         [-0.0179, -0.3904],\n",
       "         [-0.0332, -0.2790],\n",
       "         [ 0.0229, -0.1854]],\n",
       "\n",
       "        [[-0.0142, -0.4707],\n",
       "         [-0.0805, -0.2840],\n",
       "         [-0.0953, -0.3215],\n",
       "         ...,\n",
       "         [-0.0783, -0.4438],\n",
       "         [-0.0064, -0.3567],\n",
       "         [ 0.0562, -0.2425]],\n",
       "\n",
       "        [[ 0.0011, -0.4781],\n",
       "         [-0.1258, -0.2116],\n",
       "         [-0.1632, -0.2199],\n",
       "         ...,\n",
       "         [-0.1539, -0.3630],\n",
       "         [-0.0478, -0.2303],\n",
       "         [-0.0302, -0.1453]]], device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=2, bias=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.base_model.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model will be saved at saved_models/bert-base-cased_cb_apt.pt\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   0%|          | 0/15 [00:04<?, ?batch/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected target size [16, 3], got [16]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-2a1b5369e491>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m }\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/aditya/project/Adapter_based_Prompt_Tuning/train.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(config_train)\u001b[0m\n\u001b[1;32m    193\u001b[0m                         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/aditya/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/aditya/.local/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1150\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[1;32m   1151\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m                                label_smoothing=self.label_smoothing)\n\u001b[0m\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/aditya/.local/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   2844\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2845\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2846\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected target size [16, 3], got [16]"
     ]
    }
   ],
   "source": [
    "config_train = {\n",
    "    \n",
    "    'dataset': dataset,\n",
    "    'dataloaders':dataloaders, \n",
    "    'model': model, \n",
    "    'device': device, \n",
    "    'criterion':criterion, \n",
    "    'optimizer':optimizer, \n",
    "    'mode':mode, \n",
    "    'scheduler': scheduler,\n",
    "    'epochs': epochs,\n",
    "    'save_checkpoint': True,\n",
    "    'checkpoint': None,\n",
    "    'model_type': model_type\n",
    "}\n",
    "\n",
    "train_model(config_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    0,  7424,    47,   304, 23136,  3121,  1886,    23,   364,  3275,\n",
       "         1075,  1992,     2,     2,   717,  3275,  1075, 10780,  1992,   480,\n",
       "        39414,  1075, 10780,  1992,  4542,     5,  1139,     9, 39414,  1075,\n",
       "           11, 15693,     4,    85,    16,  2034,   160, 21478,  1214,     8,\n",
       "           16,   540,    87,    80,   728,   108,  1656,    31,     5,   755,\n",
       "          852,     4,    85,    16,    45,    11,     5,   928, 17311,  3121,\n",
       "         1886,  2056,  7328, 39414,  1075, 21144,    50, 32699,   225,  1908,\n",
       "        15211,  4492,     4,    20,  1992,   745,    21,  4209,    11,  1125,\n",
       "           73, 10684,    19,    10,    92,   745,    19, 10867,  1065,     5,\n",
       "         1992,    36,  7048,   253,     9,  1566,   322,     2,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_object[5][0]['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
