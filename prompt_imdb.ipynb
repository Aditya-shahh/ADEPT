{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de558310-e8f6-4cdc-ab82-768e6422b1ba",
   "metadata": {},
   "source": [
    "## Prompt + head layer tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cfb7de-0d25-472c-a6dd-38c1bfe458b6",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eea1737e-fb30-4ef2-ba2d-cef537dedb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, RobertaModel\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f7026c-2ca3-46ad-afbf-f6eda7d2af79",
   "metadata": {},
   "source": [
    "### Prompt Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c9c4b89-10b8-49da-b851-286d7acc8522",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PROMPTEmbedding(nn.Module):\n",
    "    def __init__(self, \n",
    "                wte: nn.Embedding,\n",
    "                n_tokens: int = 10, \n",
    "                random_range: float = 0.5,\n",
    "                initialize_from_vocab: bool = True):\n",
    "        super(PROMPTEmbedding, self).__init__()\n",
    "        self.wte = wte\n",
    "        self.n_tokens = n_tokens\n",
    "        self.learned_embedding = nn.parameter.Parameter(self.initialize_embedding(wte,\n",
    "                                                                               n_tokens, \n",
    "                                                                               random_range, \n",
    "                                                                               initialize_from_vocab))\n",
    "            \n",
    "    def initialize_embedding(self, \n",
    "                             wte: nn.Embedding,\n",
    "                             n_tokens: int = 10, \n",
    "                             random_range: float = 0.5, \n",
    "                             initialize_from_vocab: bool = True):\n",
    "        if initialize_from_vocab:\n",
    "            return self.wte.weight[:n_tokens].clone().detach()\n",
    "        return torch.FloatTensor(wte.weight.size(1), n_tokens).uniform_(-random_range, random_range)\n",
    "            \n",
    "    def forward(self, tokens):\n",
    "        input_embedding = self.wte(tokens[:, self.n_tokens:])\n",
    "        learned_embedding = self.learned_embedding.repeat(input_embedding.size(0), 1, 1)\n",
    "        return torch.cat([learned_embedding, input_embedding], 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d54a461-7cee-43a8-a001-9bc624b99a28",
   "metadata": {},
   "source": [
    "### Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a38d3318-c6c9-449e-b34d-1c652d4e045d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Check GPU\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.enabled   = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c82afa-e18e-4ab1-96d2-30853101f1d3",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67acf85f-9068-484f-a70e-dd5e215784df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 40000, 5000, 5000, 5000, 5000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('Train.csv')\n",
    "test_data = pd.read_csv('Test.csv')\n",
    "valid_data = pd.read_csv('Valid.csv')\n",
    "\n",
    "train_text = train_data['text']\n",
    "train_labels = train_data['label']\n",
    "\n",
    "valid_text = valid_data['text']\n",
    "valid_labels = valid_data['label']\n",
    "\n",
    "test_text = test_data['text']\n",
    "test_labels = test_data['label']\n",
    "\n",
    "#print total length of dataset\n",
    "len(train_text), len(train_labels), len(test_labels), len(test_text), len(valid_text), len(valid_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce8e87b-8afb-49b8-8ef5-15a6c61d93c4",
   "metadata": {},
   "source": [
    "### Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e822a4c-085b-49e3-b59f-721b26c66f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Specify the Hyper parameters \n",
    "\n",
    "BATCH_SIZE = 16\n",
    "NUM_LABELS = 2\n",
    "EPOCHS = 15\n",
    "LEARNING_RATE = 8e-5\n",
    "N_Tokens = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cbba7e8-2a76-4703-b8d3-9fcda646d972",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load the Roberta tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0668a7-0a6b-47ae-9c92-19dd514df40f",
   "metadata": {},
   "source": [
    "### Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "864fefe8-06d6-4923-a496-19663b09ef26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    This is our custom dataset class which will load the text and their corresponding labels into Pytorch tensors\n",
    "    \"\"\"\n",
    "    def __init__(self, labels, text):\n",
    "        self.labels = labels\n",
    "        self.text = text\n",
    "        \n",
    "        self.n_tokens = N_Tokens\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {}\n",
    "        text = self.text[idx]\n",
    "\n",
    "        #Roberta Tokenizer to tokenize the text\n",
    "        inputs = tokenizer.encode_plus(text, \n",
    "                                        add_special_tokens=True,   # Adds [CLS] and [SEP] token to every input text\n",
    "                                        max_length=492, \n",
    "                                        truncation=True, \n",
    "                                        return_tensors='pt',\n",
    "                                        padding=\"max_length\")\n",
    "        \n",
    "        inputs['input_ids'] = torch.cat([torch.full((1,self.n_tokens), 5256), inputs['input_ids']], 1)\n",
    "        inputs['attention_mask'] = torch.cat([torch.full((1, self.n_tokens), 1), inputs['attention_mask']], 1)\n",
    "\n",
    "        \n",
    "        return inputs, torch.tensor(self.labels[idx])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "        \n",
    "        \n",
    "# Create train, test and val datasets\n",
    "train_data_object = Dataset(\n",
    "    labels = train_labels,\n",
    "    text = train_text,\n",
    ")\n",
    "\n",
    "test_data_object = Dataset(\n",
    "    labels = test_labels,\n",
    "    text = test_text,\n",
    ")\n",
    "\n",
    "val_data_object = Dataset(\n",
    "    labels = valid_labels,\n",
    "    text = valid_text,\n",
    ")\n",
    "\n",
    "\n",
    "## We call the dataloader class\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data_object,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    pin_memory=True,\n",
    "    num_workers= 64,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    " )\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data_object,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    pin_memory=True,\n",
    "    num_workers=64,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    " )\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_data_object,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    pin_memory=True,\n",
    "    num_workers=64,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    " )\n",
    "\n",
    "dataloaders = {'Train': train_loader, 'Test': test_loader, 'Val': val_loader}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f0869c-2274-4807-a0c3-58005f8a93a5",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64902f53-224f-48fe-9c80-a4d3959679ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, bert, num_classes):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear modules and assign them as\n",
    "        member variables.\n",
    "        \"\"\"\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.bert = bert\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.fcbert1 = nn.Linear(768, 128)\n",
    "        self.fcbert2 = nn.Linear(128, 16)\n",
    "        self.fcbert3 = nn.Linear(16, num_classes)\n",
    "\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Tensors.\n",
    "        \"\"\"\n",
    "        # 1, 768\n",
    "        output_bert = self.bert(input_ids = input_ids, attention_mask = attention_mask).pooler_output\n",
    "        \n",
    "        \n",
    "        output = self.dropout(F.leaky_relu(self.fcbert1(output_bert), .1))    #1, 128\n",
    "         \n",
    "        output = self.dropout(F.leaky_relu(self.fcbert2(output), 0.1))     #1, 16\n",
    "        \n",
    "        output = self.fcbert3(output)    #1, 3\n",
    "         \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d96efce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', \n",
    "                                                         num_labels=2,\n",
    "                                                         output_attentions=False,\n",
    "                                                         output_hidden_states=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc67219",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9975349-56ca-4cd7-a029-ea690494ea8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for param in model.roberta.parameters():\n",
    "    param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88026915-4317-482d-965b-1ba6fd0b3823",
   "metadata": {},
   "source": [
    "### Add prompt embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "011b9d71-09a2-4a0c-b9d6-dd608da480c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PROMPTEmbedding(\n",
       "  (wte): Embedding(50265, 768, padding_idx=1)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_emb = PROMPTEmbedding(model.roberta.get_input_embeddings(), \n",
    "                      n_tokens= N_Tokens, \n",
    "                      initialize_from_vocab=True)\n",
    "\n",
    "prompt_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4202dd-e2a5-4df4-812b-fbfccec3df0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.roberta.set_input_embeddings(prompt_emb)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ccdcf7-2fbf-4b22-9fd5-57642c2d5ed4",
   "metadata": {},
   "source": [
    "### few checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6b288cb-235a-48b2-b2e0-da43bf028aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 768])\n",
      "\n",
      "bert.embeddings.word_embeddings.learned_embedding tensor([[ 0.1476, -0.0365,  0.0753,  ..., -0.0023,  0.0172, -0.0016],\n",
      "        [ 0.0156,  0.0076, -0.0118,  ..., -0.0022,  0.0081, -0.0156],\n",
      "        [-0.0347, -0.0873, -0.0180,  ...,  0.1174, -0.0098, -0.0355],\n",
      "        ...,\n",
      "        [-0.1332, -0.0391, -0.0661,  ..., -0.0450, -0.0546,  0.0156],\n",
      "        [ 0.0358,  0.0647, -0.1526,  ..., -0.1164, -0.0242, -0.0792],\n",
      "        [-0.0911, -0.1117, -0.0304,  ..., -0.0569, -0.1113, -0.1200]],\n",
      "       device='cuda:0')\n",
      "fcbert1.weight tensor([[ 0.0113, -0.0122, -0.0056,  ...,  0.0317,  0.0335,  0.0181],\n",
      "        [ 0.0041,  0.0299,  0.0168,  ...,  0.0254,  0.0282, -0.0151],\n",
      "        [-0.0133, -0.0197,  0.0121,  ..., -0.0112, -0.0184,  0.0258],\n",
      "        ...,\n",
      "        [ 0.0023,  0.0245,  0.0175,  ...,  0.0328,  0.0109, -0.0280],\n",
      "        [-0.0139, -0.0232, -0.0149,  ...,  0.0152,  0.0285, -0.0148],\n",
      "        [-0.0111, -0.0249, -0.0346,  ..., -0.0162,  0.0322,  0.0188]],\n",
      "       device='cuda:0')\n",
      "fcbert1.bias tensor([-0.0203,  0.0209,  0.0125, -0.0188,  0.0021,  0.0058,  0.0259, -0.0196,\n",
      "        -0.0034, -0.0289, -0.0030,  0.0242,  0.0093,  0.0002,  0.0116, -0.0253,\n",
      "        -0.0273, -0.0224,  0.0007, -0.0300, -0.0190,  0.0053, -0.0211, -0.0276,\n",
      "        -0.0180,  0.0327,  0.0112, -0.0223, -0.0333, -0.0199,  0.0053, -0.0145,\n",
      "         0.0191, -0.0204,  0.0201,  0.0024,  0.0029, -0.0102, -0.0046,  0.0221,\n",
      "         0.0215,  0.0234, -0.0169, -0.0354,  0.0226, -0.0319, -0.0281, -0.0147,\n",
      "        -0.0167, -0.0329, -0.0282,  0.0294,  0.0079, -0.0353, -0.0031,  0.0326,\n",
      "         0.0005,  0.0154,  0.0100,  0.0238,  0.0313,  0.0122,  0.0041, -0.0123,\n",
      "         0.0233,  0.0316,  0.0244, -0.0197, -0.0284, -0.0164, -0.0127,  0.0137,\n",
      "         0.0014, -0.0139,  0.0242, -0.0126,  0.0084, -0.0015,  0.0068, -0.0257,\n",
      "        -0.0131, -0.0068, -0.0325,  0.0063, -0.0213, -0.0146, -0.0298,  0.0134,\n",
      "         0.0358, -0.0186, -0.0147,  0.0134, -0.0027,  0.0028, -0.0081, -0.0354,\n",
      "        -0.0195, -0.0033,  0.0050, -0.0026, -0.0315, -0.0031, -0.0050,  0.0253,\n",
      "        -0.0057, -0.0189, -0.0351, -0.0323,  0.0211,  0.0316, -0.0152, -0.0348,\n",
      "         0.0097,  0.0061, -0.0335,  0.0090,  0.0077,  0.0244, -0.0053,  0.0051,\n",
      "         0.0329,  0.0328, -0.0187,  0.0241, -0.0264,  0.0216, -0.0078, -0.0182],\n",
      "       device='cuda:0')\n",
      "fcbert2.weight tensor([[ 0.0630,  0.0136, -0.0029,  ..., -0.0560, -0.0560,  0.0689],\n",
      "        [ 0.0779, -0.0306,  0.0701,  ..., -0.0781,  0.0381,  0.0639],\n",
      "        [-0.0252, -0.0809, -0.0793,  ...,  0.0360, -0.0142,  0.0417],\n",
      "        ...,\n",
      "        [-0.0740,  0.0598, -0.0316,  ..., -0.0772,  0.0122, -0.0328],\n",
      "        [-0.0367,  0.0047, -0.0726,  ...,  0.0066,  0.0186, -0.0722],\n",
      "        [ 0.0137, -0.0333, -0.0853,  ..., -0.0174,  0.0186,  0.0081]],\n",
      "       device='cuda:0')\n",
      "fcbert2.bias tensor([-0.0307, -0.0193, -0.0345, -0.0536,  0.0099, -0.0826, -0.0161,  0.0138,\n",
      "         0.0486,  0.0312, -0.0541,  0.0689, -0.0419,  0.0099, -0.0309, -0.0662],\n",
      "       device='cuda:0')\n",
      "fcbert3.weight tensor([[-0.1115,  0.0711,  0.0102, -0.1578,  0.2040, -0.0766, -0.0342, -0.0828,\n",
      "          0.1235, -0.1344, -0.1012,  0.1948, -0.1159,  0.1997,  0.2400, -0.1809],\n",
      "        [-0.1611, -0.0224, -0.2474, -0.0523,  0.0340,  0.0413, -0.2028,  0.0984,\n",
      "         -0.0647,  0.2409,  0.0622,  0.0401, -0.1669,  0.1569,  0.1902,  0.1660]],\n",
      "       device='cuda:0')\n",
      "fcbert3.bias tensor([-0.1030,  0.0938], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(model.bert.embeddings.word_embeddings.learned_embedding.shape)\n",
    "print()\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4690d780-dd45-49b1-a87b-b27aefe90c87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' prompted prompted prompted prompted prompted prompted prompted prompted prompted prompted prompted prompted prompted prompted prompted prompted prompted prompted prompted prompted<s>Usually musicals in the 1940\\'s were of a set formula - and if you studied films you know what I\\'m talking about - a certain running lenghth, very \"showy\" performances that were great on the surface but never got into the real personalities of the characters etc.<br /><br />THIS ONE IS DIFFERENT - and light years better and well worth it\\'s nomination for best picture of the year - 1945 (although had no chance of beating the eventual winner - Lost Weekend).<br /><br />Gene Kelly was probably in the best form of his career - yes I know about \"American in Paris\" and \"Singing in the Rain\". This one is different. He really gets into his character of a \"sea wolf\" thinking (at first) that \"picking up any girl while on leave\" is nothing more than a lark. And if you had to make up a \"story\" to get her - so be it - until. Sort of like the Music Man when he gets \"his foot caught in the door\". The eventual hilarity of the film stems mostly from his and his new pal (Sinatra)\\'s attempt to make the \"story\" good in order to \"get the girl\" that he REALLY and unexpectedly falls in love with. You are going to have to see the movie to see what I mean.<br /><br />Besides that there are so many other elements of great film in this one, it\\'s a classic buddy story, nostalgia to a time when WWII was almost over (the war ended about a month after the films release), a realization that a guy that always laughed at life can find out that he really is a great human being, great songs and probably a few other elements of classic film making that I can\\'t think of right now.<br /><br />Why not a 10? Near the end - at nearly 2 1/2 hours starts to feel a bit long. There is a small ballet number that Gene Kelly does that must have been a sensation in 1945 but seems dated and feels like it just adds minutes now. But overall, this ones a definite winner on every level.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print input check\n",
    "tokenizer.decode(train_data_object[700][0]['input_ids'].detach().cpu().numpy().tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2d4b4d1-6070-430d-8b4b-329a15d7572c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#optimizer\n",
    "optimizer = AdamW(model.parameters(), lr = LEARNING_RATE, eps=1e-8)\n",
    "\n",
    "#Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Defining LR Scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=0, \n",
    "    num_training_steps=len(train_loader)*EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab62a1dd-1270-4772-b291-89380ecb5e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------+------------+\n",
      "|                      Modules                      | Parameters |\n",
      "+---------------------------------------------------+------------+\n",
      "| bert.embeddings.word_embeddings.learned_embedding |   15360    |\n",
      "|                   fcbert1.weight                  |   98304    |\n",
      "|                    fcbert1.bias                   |    128     |\n",
      "|                   fcbert2.weight                  |    2048    |\n",
      "|                    fcbert2.bias                   |     16     |\n",
      "|                   fcbert3.weight                  |     32     |\n",
      "|                    fcbert3.bias                   |     2      |\n",
      "+---------------------------------------------------+------------+\n",
      "Total Trainable Params: 115890\n"
     ]
    }
   ],
   "source": [
    "#Print trainable parameters\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        param = parameter.numel()\n",
    "        table.add_row([name, param])\n",
    "        total_params+=param\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    \n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e44069d-bd2e-49b5-9de2-ed59d706e843",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to calculate accuracy\n",
    "\n",
    "def get_accuracy(preds, labels):\n",
    "  total_acc = 0.0\n",
    "  \n",
    "  for i in range(len(labels)):\n",
    "    if labels[i] == preds[i]:\n",
    "      total_acc+=1.0\n",
    "  \n",
    "  return total_acc / len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56643726-6647-4ad6-a36e-e5e99cf46cfc",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a884fbad-64e4-4e5d-b725-330a26062796",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_valid_f1 = 0.0\n",
    "PATH = 'prompt_head_ft_imdb.pt'\n",
    "\n",
    "for epoch in range(0, EPOCHS):\n",
    "  \n",
    "\n",
    "    print('-'*10)\n",
    "    print('Epoch {}/{}'.format(epoch+1, EPOCHS))\n",
    "\n",
    "    for phase in ['Train', 'Val']:\n",
    "\n",
    "        batch_loss = 0.0000   #live loss\n",
    "        batch_acc = 0.0000   #live accuracy\n",
    "\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "\n",
    "        if phase == 'Train':\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "        \n",
    "        with tqdm(dataloaders[phase], unit=\"batch\", desc=phase) as tepoch:\n",
    "\n",
    "          for idx, (data, labels) in enumerate(tepoch):\n",
    "            input_ids =  data['input_ids'].squeeze(1).to(device)\n",
    "            attention_mask = data['attention_mask'].squeeze(1).to(device)\n",
    "            \n",
    "            \n",
    "            labels = labels.to(device)\n",
    "\n",
    "            output = model(input_ids = input_ids, attention_mask = attention_mask)\n",
    "\n",
    "            loss = criterion(output, labels)\n",
    "\n",
    "            if phase == 'Train':\n",
    "\n",
    "                #zero gradients\n",
    "                optimizer.zero_grad() \n",
    "\n",
    "                # Backward pass  (calculates the gradients)\n",
    "                loss.backward()   \n",
    "\n",
    "                optimizer.step()             # Updates the weights\n",
    "                \n",
    "                scheduler.step()\n",
    "                \n",
    "                \n",
    "            batch_loss += loss.item()\n",
    "                \n",
    "            _, preds = output.data.max(1)\n",
    "            y_pred.extend(preds.tolist())\n",
    "            y_true.extend(labels.tolist())\n",
    "            \n",
    "            batch_acc = get_accuracy(y_pred, y_true)\n",
    "            \n",
    "            tepoch.set_postfix(loss = batch_loss/(idx+1), accuracy = batch_acc )\n",
    "\n",
    "          pre = precision_score(y_true, y_pred, average='weighted')\n",
    "          recall = recall_score(y_true, y_pred, average='weighted')\n",
    "          f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "          \n",
    "\n",
    "          print(\"F1: {:.4f}, Precision: {:.4f}, Recall : {:.4f}.\".format(f1, pre, recall))\n",
    "        \n",
    "          if phase == 'Val':\n",
    "            if f1 > best_valid_f1:\n",
    "                best_valid_f1 = f1\n",
    "                torch.save(model.state_dict(), PATH)\n",
    "                print('Model Saved!')\n",
    "        \n",
    "          print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0ebdc77-8742-491f-99c6-d52419ecb49a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('prompt_head_ft_imdb.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c7fed4-ac93-4a84-93bd-c11647075899",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a349232-85d0-4b78-8952-7aee9905d3f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 312/312 [01:03<00:00,  4.91batch/s, accuracy=0.926, loss=0.214]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "F1: 0.926482, Precision: 0.926483, Recall : 0.926485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "batch_loss = 0.0   #batch loss\n",
    "batch_acc = 0.0   #batch accuracy\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "# set the model to evaluation mode            \n",
    "model.eval()\n",
    "\n",
    "phase = 'Test'\n",
    "\n",
    "with tqdm(test_loader, unit=\"batch\", desc=phase) as tepoch:\n",
    "    \n",
    "    for idx, (data, labels) in enumerate(tepoch):\n",
    "        \n",
    "        input_ids =  data['input_ids'].squeeze(1).to(device)\n",
    "        attention_mask = data['attention_mask'].squeeze(1).to(device)\n",
    "\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "\n",
    "            output = model(input_ids = input_ids, attention_mask = attention_mask)\n",
    "\n",
    "            loss = criterion(output, labels)\n",
    "            \n",
    "            _, preds = output.data.max(1)\n",
    "            y_pred.extend(preds.tolist())\n",
    "            y_true.extend(labels.tolist())\n",
    "            \n",
    "            batch_acc = get_accuracy(y_pred, y_true)\n",
    "            batch_loss += loss.item()\n",
    "            \n",
    "        tepoch.set_postfix(loss = batch_loss/(idx+1), accuracy = batch_acc )\n",
    "\n",
    "\n",
    "pre = precision_score(y_true, y_pred, average='macro')\n",
    "recall = recall_score(y_true, y_pred, average='macro')\n",
    "f1 = f1_score(y_true, y_pred, average='macro')\n",
    "print(\"\")\n",
    "\n",
    "print(\"F1: {:.6f}, Precision: {:.6f}, Recall : {:.6f}\".format(f1, pre, recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e477c0-b84b-4f7a-ba00-c7ba55da6671",
   "metadata": {},
   "source": [
    "## Only prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cc1bae-1a66-4d8b-8b55-945f3b7348be",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2fa1c04a-56a4-4493-905a-a25d77ea4692",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe554f05-baa5-47af-87cf-875f040735ac",
   "metadata": {},
   "source": [
    "### prompts emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eea5b77b-30b9-4d8f-9357-58e1be09e151",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PROMPTEmbedding(nn.Module):\n",
    "    def __init__(self, \n",
    "                wte: nn.Embedding,\n",
    "                n_tokens: int = 10, \n",
    "                random_range: float = 0.5,\n",
    "                initialize_from_vocab: bool = True):\n",
    "        super(PROMPTEmbedding, self).__init__()\n",
    "        self.wte = wte\n",
    "        self.n_tokens = n_tokens\n",
    "        self.learned_embedding = nn.parameter.Parameter(self.initialize_embedding(wte,\n",
    "                                                                               n_tokens, \n",
    "                                                                               random_range, \n",
    "                                                                               initialize_from_vocab))\n",
    "            \n",
    "    def initialize_embedding(self, \n",
    "                             wte: nn.Embedding,\n",
    "                             n_tokens: int = 10, \n",
    "                             random_range: float = 0.5, \n",
    "                             initialize_from_vocab: bool = True):\n",
    "        if initialize_from_vocab:\n",
    "            return self.wte.weight[:n_tokens].clone().detach()\n",
    "        return torch.FloatTensor(wte.weight.size(1), n_tokens).uniform_(-random_range, random_range)\n",
    "            \n",
    "    def forward(self, tokens):\n",
    "        input_embedding = self.wte(tokens[:, self.n_tokens:])\n",
    "        learned_embedding = self.learned_embedding.repeat(input_embedding.size(0), 1, 1)\n",
    "        return torch.cat([learned_embedding, input_embedding], 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "deb387d4-4945-473b-806b-9d5b2e60b771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check GPU\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.enabled   = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1be16d6-e11d-43bc-8ed4-e5961440699d",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0b0a959-05ed-488d-ae16-357fb8877620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 40000, 5000, 5000, 5000, 5000)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('Train.csv')\n",
    "test_data = pd.read_csv('Test.csv')\n",
    "valid_data = pd.read_csv('Valid.csv')\n",
    "\n",
    "train_text = train_data['text']\n",
    "train_labels = train_data['label']\n",
    "\n",
    "valid_text = valid_data['text']\n",
    "valid_labels = valid_data['label']\n",
    "\n",
    "test_text = test_data['text']\n",
    "test_labels = test_data['label']\n",
    "\n",
    "#print total length of dataset\n",
    "len(train_text), len(train_labels), len(test_labels), len(test_text), len(valid_text), len(valid_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18359fc8-5ba7-4dda-a8b4-771eb5904a60",
   "metadata": {},
   "source": [
    "### Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "37998ca5-c551-4cce-a298-0aed2f8b7815",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Specify the Hyper parameters \n",
    "\n",
    "BATCH_SIZE = 16\n",
    "NUM_LABELS = 2  # since we have two labels -  positive and negative i.e 0 and 1\n",
    "EPOCHS = 15\n",
    "LEARNING_RATE = 1e-4\n",
    "N_Tokens = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a78a90ed-68ec-4ba8-81ff-0b96253f265a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load the Roberta tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d86bce2-54d9-401e-8cf6-65477260d517",
   "metadata": {},
   "source": [
    "### Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6e5c3c37-9f5c-49a0-9659-c9645199c5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    This is our custom dataset class which will load the text and their corresponding labels into Pytorch tensors\n",
    "    \"\"\"\n",
    "    def __init__(self, labels, text):\n",
    "        self.labels = labels\n",
    "        self.text = text\n",
    "        \n",
    "        self.n_tokens = N_Tokens\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {}\n",
    "        text = self.text[idx]\n",
    "\n",
    "        #Roberta Tokenizer to tokenize the text\n",
    "        inputs = tokenizer.encode_plus(text, \n",
    "                                        add_special_tokens=True,   # Adds [CLS] and [SEP] token to every input text\n",
    "                                        max_length=492, \n",
    "                                        truncation=True, \n",
    "                                        return_tensors='pt',\n",
    "                                        padding=\"max_length\")\n",
    "        \n",
    "        inputs['input_ids'] = torch.cat([torch.full((1,self.n_tokens), 5256), inputs['input_ids']], 1)\n",
    "        inputs['attention_mask'] = torch.cat([torch.full((1, self.n_tokens), 1), inputs['attention_mask']], 1)\n",
    "\n",
    "        \n",
    "        return inputs, torch.tensor(self.labels[idx])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "        \n",
    "        \n",
    "# Create train, test and val datasets\n",
    "train_data_object = Dataset(\n",
    "    labels = train_labels,\n",
    "    text = train_text,\n",
    ")\n",
    "\n",
    "test_data_object = Dataset(\n",
    "    labels = test_labels,\n",
    "    text = test_text,\n",
    ")\n",
    "\n",
    "val_data_object = Dataset(\n",
    "    labels = valid_labels,\n",
    "    text = valid_text,\n",
    ")\n",
    "\n",
    "\n",
    "## We call the dataloader class\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data_object,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    pin_memory=True,\n",
    "    num_workers= 64,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    " )\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data_object,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    pin_memory=True,\n",
    "    num_workers=64,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    " )\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_data_object,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    pin_memory=True,\n",
    "    num_workers=64,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    " )\n",
    "\n",
    "dataloaders = {'Train': train_loader, 'Test': test_loader, 'Val': val_loader}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68bd8a3-6a59-4940-a494-5b380b3021b1",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1015991b-2941-43cf-9b4a-82cd6e3aca0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', \n",
    "                                                         num_labels=NUM_LABELS,\n",
    "                                                         output_attentions=False,\n",
    "                                                         output_hidden_states=False)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf02b5b3-d012-4eb1-9314-babb2197b729",
   "metadata": {},
   "source": [
    "### Add prompt emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "09c99b5f-d64a-4cd0-b1ba-4c1faca99612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPTEmbedding(\n",
      "  (wte): Embedding(50265, 768, padding_idx=1)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 768])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_emb = PROMPTEmbedding(model.get_input_embeddings(), \n",
    "                      n_tokens= N_Tokens, \n",
    "                      initialize_from_vocab=True)\n",
    "\n",
    "print(prompt_emb)\n",
    "\n",
    "model.set_input_embeddings(prompt_emb)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "model.roberta.embeddings.word_embeddings.learned_embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05219243-0e01-497a-bfde-de983f821583",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "42eed1d3-343c-4992-a762-5a503b4d4f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#optimizer\n",
    "optimizer = AdamW([model.roberta.embeddings.word_embeddings.learned_embedding], lr = LEARNING_RATE, eps=1e-8)\n",
    "\n",
    "#Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Defining LR Scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=0, \n",
    "    num_training_steps=len(train_loader)*EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a4449356-1c70-4622-b3f3-a64934540a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------+------------+\n",
      "|                       Modules                        | Parameters |\n",
      "+------------------------------------------------------+------------+\n",
      "| roberta.embeddings.word_embeddings.learned_embedding |   15360    |\n",
      "+------------------------------------------------------+------------+\n",
      "Total Trainable Params: 15360\n"
     ]
    }
   ],
   "source": [
    "#Print trainable parameters\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        param = parameter.numel()\n",
    "        table.add_row([name, param])\n",
    "        total_params+=param\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    \n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26d7e62-8606-443b-aeb4-9cf14017d951",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0a2cd30a-2f54-4f3e-b0db-1b36c2b9b290",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to calculate accuracy\n",
    "\n",
    "def get_accuracy(preds, labels):\n",
    "  total_acc = 0.0\n",
    "  \n",
    "  for i in range(len(labels)):\n",
    "    if labels[i] == preds[i]:\n",
    "      total_acc+=1.0\n",
    "  \n",
    "  return total_acc / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735f0026-0d6e-4fff-b467-d6753c88f54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_valid_f1 = 0.0\n",
    "PATH = 'only_prompt_imdb.pt'\n",
    "\n",
    "for epoch in range(0, EPOCHS):\n",
    "  \n",
    "\n",
    "    print('-'*10)\n",
    "    print('Epoch {}/{}'.format(epoch+1, EPOCHS))\n",
    "\n",
    "    for phase in ['Train', 'Val']:\n",
    "\n",
    "        batch_loss = 0.0000   #live loss\n",
    "        batch_acc = 0.0000   #live accuracy\n",
    "\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "\n",
    "        if phase == 'Train':\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "        \n",
    "        with tqdm(dataloaders[phase], unit=\"batch\", desc=phase) as tepoch:\n",
    "\n",
    "          for idx, (data, labels) in enumerate(tepoch):\n",
    "            input_ids =  data['input_ids'].squeeze(1).to(device)\n",
    "            attention_mask = data['attention_mask'].squeeze(1).to(device)\n",
    "            \n",
    "            labels = labels.to(device)\n",
    "\n",
    "            output = model(input_ids = input_ids, attention_mask = attention_mask).logits\n",
    "\n",
    "            loss = criterion(output, labels)\n",
    "\n",
    "            if phase == 'Train':\n",
    "\n",
    "                #zero gradients\n",
    "                optimizer.zero_grad() \n",
    "\n",
    "                # Backward pass  (calculates the gradients)\n",
    "                loss.backward()   \n",
    "\n",
    "                optimizer.step()             # Updates the weights\n",
    "                \n",
    "                scheduler.step()\n",
    "                \n",
    "                \n",
    "            batch_loss += loss.item()\n",
    "                \n",
    "            _, preds = output.data.max(1)\n",
    "            y_pred.extend(preds.tolist())\n",
    "            y_true.extend(labels.tolist())\n",
    "            \n",
    "            batch_acc = get_accuracy(y_pred, y_true)\n",
    "            \n",
    "            tepoch.set_postfix(loss = batch_loss/(idx+1), accuracy = batch_acc )\n",
    "\n",
    "          pre = precision_score(y_true, y_pred, average='weighted')\n",
    "          recall = recall_score(y_true, y_pred, average='weighted')\n",
    "          f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "        \n",
    "          print(\"F1: {:.4f}, Precision: {:.4f}, Recall : {:.4f}.\".format(f1, pre, recall))\n",
    "        \n",
    "        \n",
    "                    \n",
    "          if phase == 'Val':\n",
    "            if f1 > best_valid_f1:\n",
    "                best_valid_f1 = f1\n",
    "                torch.save(model.state_dict(), PATH)\n",
    "                print('Model Saved!')\n",
    "        \n",
    "          print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ea3e6b-f99d-4ab8-af9c-a1a43b5b8945",
   "metadata": {},
   "source": [
    "### train few more epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28b7a51-c2f9-4279-a4a4-6a4753879ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_valid_f1 = 0.8069\n",
    "PATH = 'only_prompt_imdb.pt'\n",
    "\n",
    "model.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb7a3c1-99ec-49c4-bee7-2e8eaa397e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For IMDB datasets\n",
    "\n",
    "for epoch in range(0, EPOCHS):\n",
    "  \n",
    "\n",
    "    print('-'*10)\n",
    "    print('Epoch {}/{}'.format(epoch+1, EPOCHS))\n",
    "\n",
    "    for phase in ['Train', 'Val']:\n",
    "\n",
    "        batch_loss = 0.0000   #live loss\n",
    "        batch_acc = 0.0000   #live accuracy\n",
    "\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "\n",
    "        if phase == 'Train':\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "        \n",
    "        with tqdm(dataloaders[phase], unit=\"batch\", desc=phase) as tepoch:\n",
    "\n",
    "          for idx, (data, labels) in enumerate(tepoch):\n",
    "            input_ids =  data['input_ids'].squeeze(1).to(device)\n",
    "            attention_mask = data['attention_mask'].squeeze(1).to(device)\n",
    "            \n",
    "            labels = labels.to(device)\n",
    "\n",
    "            output = model(input_ids = input_ids, attention_mask = attention_mask).logits\n",
    "\n",
    "            loss = criterion(output, labels)\n",
    "\n",
    "            if phase == 'Train':\n",
    "\n",
    "                #zero gradients\n",
    "                optimizer.zero_grad() \n",
    "\n",
    "                # Backward pass  (calculates the gradients)\n",
    "                loss.backward()   \n",
    "\n",
    "                optimizer.step()             # Updates the weights\n",
    "                \n",
    "                scheduler.step()\n",
    "                \n",
    "                \n",
    "            batch_loss += loss.item()\n",
    "                \n",
    "            _, preds = output.data.max(1)\n",
    "            y_pred.extend(preds.tolist())\n",
    "            y_true.extend(labels.tolist())\n",
    "            \n",
    "            batch_acc = get_accuracy(y_pred, y_true)\n",
    "            \n",
    "            tepoch.set_postfix(loss = batch_loss/(idx+1), accuracy = batch_acc )\n",
    "        \n",
    "\n",
    "\n",
    "          print(confusion_matrix(y_true, y_pred))\n",
    "          pre = precision_score(y_true, y_pred, average='weighted')\n",
    "          recall = recall_score(y_true, y_pred, average='weighted')\n",
    "          f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "        \n",
    "          print(\"F1: {:.4f}, Precision: {:.4f}, Recall : {:.4f}.\".format(f1, pre, recall))\n",
    "        \n",
    "        \n",
    "                    \n",
    "          if phase == 'Val':\n",
    "            if f1 > best_valid_f1:\n",
    "                best_valid_f1 = f1\n",
    "                torch.save(model.state_dict(), PATH)\n",
    "                print('Model Saved!')\n",
    "        \n",
    "          print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeabb0c8-929b-4ba9-99d4-ab9a0b20d137",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f537e3ac-3d34-4b95-9f78-530ce749fd91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('only_prompt_imdb.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cb29007a-6f8b-4eb2-8b4c-8cae21c9a48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 312/312 [01:03<00:00,  4.90batch/s, accuracy=0.893, loss=0.327]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "F1: 0.893024, Precision: 0.893075, Recall : 0.893022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "batch_loss = 0.0   #batch loss\n",
    "batch_acc = 0.0   #batch accuracy\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "# set the model to evaluation mode            \n",
    "model.eval()\n",
    "\n",
    "phase = 'Test'\n",
    "\n",
    "with tqdm(test_loader, unit=\"batch\", desc=phase) as tepoch:\n",
    "    \n",
    "    for idx, (data, labels) in enumerate(tepoch):\n",
    "        \n",
    "        input_ids =  data['input_ids'].squeeze(1).to(device)\n",
    "        attention_mask = data['attention_mask'].squeeze(1).to(device)\n",
    "\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "\n",
    "            output = model(input_ids = input_ids, attention_mask = attention_mask).logits\n",
    "\n",
    "            loss = criterion(output, labels)\n",
    "            \n",
    "            _, preds = output.data.max(1)\n",
    "            y_pred.extend(preds.tolist())\n",
    "            y_true.extend(labels.tolist())\n",
    "            \n",
    "            batch_acc = get_accuracy(y_pred, y_true)\n",
    "            batch_loss += loss.item()\n",
    "            \n",
    "        tepoch.set_postfix(loss = batch_loss/(idx+1), accuracy = batch_acc )\n",
    "\n",
    "pre = precision_score(y_true, y_pred, average='macro')\n",
    "recall = recall_score(y_true, y_pred, average='macro')\n",
    "f1 = f1_score(y_true, y_pred, average='macro')\n",
    "print(\"\")\n",
    "\n",
    "print(\"F1: {:.6f}, Precision: {:.6f}, Recall : {:.6f}\".format(f1, pre, recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5079f3-5df0-4f62-b8f5-cd33b290480d",
   "metadata": {},
   "source": [
    "## Prompt + Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "59fc8db7-b866-42d6-8a8e-760d97ed3be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9699aabf-f683-47c6-9d15-f9a8797573bc",
   "metadata": {},
   "source": [
    "### Prompt Embeddding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cd7a8b98-0e1a-48cc-b5b4-1c8c7169f438",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PROMPTEmbedding(nn.Module):\n",
    "    def __init__(self, \n",
    "                wte: nn.Embedding,\n",
    "                n_tokens: int = 10, \n",
    "                random_range: float = 0.5,\n",
    "                initialize_from_vocab: bool = True):\n",
    "        super(PROMPTEmbedding, self).__init__()\n",
    "        self.wte = wte\n",
    "        self.n_tokens = n_tokens\n",
    "        self.learned_embedding = nn.parameter.Parameter(self.initialize_embedding(wte,\n",
    "                                                                               n_tokens, \n",
    "                                                                               random_range, \n",
    "                                                                               initialize_from_vocab))\n",
    "            \n",
    "    def initialize_embedding(self, \n",
    "                             wte: nn.Embedding,\n",
    "                             n_tokens: int = 10, \n",
    "                             random_range: float = 0.5, \n",
    "                             initialize_from_vocab: bool = True):\n",
    "        if initialize_from_vocab:\n",
    "            return self.wte.weight[:n_tokens].clone().detach()\n",
    "        return torch.FloatTensor(wte.weight.size(1), n_tokens).uniform_(-random_range, random_range)\n",
    "            \n",
    "    def forward(self, tokens):\n",
    "        input_embedding = self.wte(tokens[:, self.n_tokens:])\n",
    "        learned_embedding = self.learned_embedding.repeat(input_embedding.size(0), 1, 1)\n",
    "        return torch.cat([learned_embedding, input_embedding], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6c0997a4-d0e3-4adb-be23-075de983ed26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check GPU\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.enabled   = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd67952-e65b-4f71-aeb3-d06f7cfb0cd9",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ce1191a2-de3c-4591-afd2-f38b9629cc9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 40000, 5000, 5000, 5000, 5000)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('Train.csv')\n",
    "test_data = pd.read_csv('Test.csv')\n",
    "valid_data = pd.read_csv('Valid.csv')\n",
    "\n",
    "train_text = train_data['text']\n",
    "train_labels = train_data['label']\n",
    "\n",
    "valid_text = valid_data['text']\n",
    "valid_labels = valid_data['label']\n",
    "\n",
    "test_text = test_data['text']\n",
    "test_labels = test_data['label']\n",
    "\n",
    "#print total length of dataset\n",
    "len(train_text), len(train_labels), len(test_labels), len(test_text), len(valid_text), len(valid_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f935177-afaa-496a-9ff8-ee623e63c9c8",
   "metadata": {},
   "source": [
    "\n",
    "### Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "03b50cb3-05db-4069-b0f1-9d2204a62ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Specify the Hyper parameters \n",
    "\n",
    "BATCH_SIZE = 16\n",
    "NUM_LABELS = 2   # since we have two labels -  positive and negative i.e 0 and 1\n",
    "EPOCHS = 15\n",
    "LEARNING_RATE = 1e-4\n",
    "N_Tokens = 20\n",
    "\n",
    "\n",
    "adapter_hidden = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "15b92b22-ed1c-4300-8054-3221b2ea1921",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load the Roberta tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "25220dca-95d9-41e7-ac35-93f595ce6bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    This is our custom dataset class which will load the text and their corresponding labels into Pytorch tensors\n",
    "    \"\"\"\n",
    "    def __init__(self, labels, text):\n",
    "        self.labels = labels\n",
    "        self.text = text\n",
    "        \n",
    "        self.n_tokens = N_Tokens\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {}\n",
    "        text = self.text[idx]\n",
    "\n",
    "        #Roberta Tokenizer to tokenize the text\n",
    "        inputs = tokenizer.encode_plus(text, \n",
    "                                        add_special_tokens=True,   # Adds [CLS] and [SEP] token to every input text\n",
    "                                        max_length=492, \n",
    "                                        truncation=True, \n",
    "                                        return_tensors='pt',\n",
    "                                        padding=\"max_length\")\n",
    "        \n",
    "        inputs['input_ids'] = torch.cat([torch.full((1,self.n_tokens), 5256), inputs['input_ids']], 1)\n",
    "        inputs['attention_mask'] = torch.cat([torch.full((1, self.n_tokens), 1), inputs['attention_mask']], 1)\n",
    "\n",
    "        \n",
    "        return inputs, torch.tensor(self.labels[idx])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "        \n",
    "        \n",
    "        \n",
    "# Create train, test and val datasets\n",
    "train_data_object = Dataset(\n",
    "    labels = train_labels,\n",
    "    text = train_text,\n",
    ")\n",
    "\n",
    "test_data_object = Dataset(\n",
    "    labels = test_labels,\n",
    "    text = test_text,\n",
    ")\n",
    "\n",
    "val_data_object = Dataset(\n",
    "    labels = valid_labels,\n",
    "    text = valid_text,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "## We call the dataloader class\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data_object,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    pin_memory=True,\n",
    "    num_workers=2,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    " )\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data_object,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    pin_memory=True,\n",
    "    num_workers=2,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    " )\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_data_object,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    pin_memory=True,\n",
    "    num_workers=2,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    " )\n",
    "\n",
    "dataloaders = {'Train': train_loader, 'Test': test_loader, 'Val': val_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eed6e84d-3170-448b-b2e3-ced7f630fef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "roberta = RobertaForSequenceClassification.from_pretrained('roberta-base', \n",
    "                                                         num_labels=NUM_LABELS,\n",
    "                                                         output_attentions=False,\n",
    "                                                         output_hidden_states=False)\n",
    "\n",
    "for param in roberta.parameters():\n",
    "    param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a996d1aa-16ce-46b9-a03b-6e0de1b3d1a7",
   "metadata": {},
   "source": [
    "### Add prompt emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b03c3ded-fefe-4a26-997a-bba2fa81cff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPTEmbedding(\n",
      "  (wte): Embedding(50265, 768, padding_idx=1)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 768])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_emb = PROMPTEmbedding(roberta.get_input_embeddings(), \n",
    "                      n_tokens= N_Tokens, \n",
    "                      initialize_from_vocab=True)\n",
    "\n",
    "print(prompt_emb)\n",
    "\n",
    "roberta.set_input_embeddings(prompt_emb)\n",
    "\n",
    "\n",
    "roberta.roberta.embeddings.word_embeddings.learned_embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48015b7-f4c4-46b6-8ca4-6693811f2489",
   "metadata": {},
   "source": [
    "\n",
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d1514e33-59e4-4ada-8c81-73b183f1cad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, bert, adapter_hidden):\n",
    "\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.bert = bert\n",
    "        \n",
    "        self.adapter_1 = nn.Linear(768, adapter_hidden)\n",
    "        \n",
    "        self.adapter_2 = nn.Linear(adapter_hidden, 768)\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \n",
    "        # get output embeddings\n",
    "\n",
    "        output_embed = self.bert.roberta.embeddings(input_ids = input_ids)\n",
    "        \n",
    "        roberta_text = output_embed\n",
    "        \n",
    "        \n",
    "        # pass the output of embeddings into first 4 encoder layers\n",
    "        for i in range(4):\n",
    "            \n",
    "            roberta_text = self.bert.roberta.encoder.layer[i](roberta_text)[0]\n",
    "            \n",
    "            \n",
    "        # pass the ouput of 4th encoder layer to adapter layers\n",
    "        roberta_text = self.adapter_1(roberta_text)\n",
    "        \n",
    "        roberta_text = self.adapter_2(roberta_text)\n",
    "        \n",
    "        \n",
    "        # output of adapter layer to 5th encoder layer and so on\n",
    "            \n",
    "        for i in range(4, 12):\n",
    "            roberta_text = self.bert.roberta.encoder.layer[i](roberta_text)[0]\n",
    "        \n",
    "        \n",
    "        # final output to classifier head\n",
    "        output = self.bert.classifier(roberta_text)\n",
    "        \n",
    "        # return final oitput\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2ce76b45-929f-4118-96b2-5ee5a9ad0761",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(roberta, adapter_hidden).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1dcbbba3-4745-4714-969d-ec699011e6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------+------------+\n",
      "|                          Modules                          | Parameters |\n",
      "+-----------------------------------------------------------+------------+\n",
      "| bert.roberta.embeddings.word_embeddings.learned_embedding |   15360    |\n",
      "|                      adapter_1.weight                     |   12288    |\n",
      "|                       adapter_1.bias                      |     16     |\n",
      "|                      adapter_2.weight                     |   12288    |\n",
      "|                       adapter_2.bias                      |    768     |\n",
      "+-----------------------------------------------------------+------------+\n",
      "Total Trainable Params: 40720\n"
     ]
    }
   ],
   "source": [
    "#Print trainable parameters\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        param = parameter.numel()\n",
    "        table.add_row([name, param])\n",
    "        total_params+=param\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    \n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3bb0e216-09fc-4030-9735-5b9c4ee72f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#optimizer\n",
    "optimizer = AdamW(model.parameters(), lr = LEARNING_RATE, eps=1e-8)\n",
    "\n",
    "#Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Defining LR Scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=0, \n",
    "    num_training_steps=len(train_loader)*EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e0b4b3fb-b2f3-4386-8b3c-40b903939261",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to calculate accuracy\n",
    "\n",
    "def get_accuracy(preds, labels):\n",
    "  total_acc = 0.0\n",
    "  \n",
    "  for i in range(len(labels)):\n",
    "    if labels[i] == preds[i]:\n",
    "      total_acc+=1.0\n",
    "  \n",
    "  return total_acc / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb87b77-c0c0-4e57-9912-c4f1442ac4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test pass\n",
    "\n",
    "\n",
    "for (data, labels) in train_loader:\n",
    "\n",
    "    input_ids =  data['input_ids'].squeeze(1).to(device)\n",
    "    attention_mask = data['attention_mask'].squeeze(1).to(device)\n",
    "\n",
    "\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    output = model(input_ids = input_ids, attention_mask = attention_mask)\n",
    "    \n",
    "    print(output)\n",
    "    \n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043ee6c9-7cc6-4ce7-91b4-6edfc3505220",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_valid_f1 = 0.0\n",
    "PATH = 'prompt+adapter_imdb.pt'\n",
    "\n",
    "for epoch in range(0, EPOCHS):\n",
    "  \n",
    "\n",
    "    print('-'*10)\n",
    "    print('Epoch {}/{}'.format(epoch+1, EPOCHS))\n",
    "\n",
    "    for phase in ['Train', 'Val']:\n",
    "\n",
    "        batch_loss = 0.0000   #live loss\n",
    "        batch_acc = 0.0000   #live accuracy\n",
    "\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "\n",
    "        if phase == 'Train':\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "        \n",
    "        with tqdm(dataloaders[phase], unit=\"batch\", desc=phase) as tepoch:\n",
    "\n",
    "          for idx, (data, labels) in enumerate(tepoch):\n",
    "            input_ids =  data['input_ids'].squeeze(1).to(device)\n",
    "            attention_mask = data['attention_mask'].squeeze(1).to(device)\n",
    "            \n",
    "            labels = labels.to(device)\n",
    "\n",
    "            output = model(input_ids = input_ids, attention_mask = attention_mask)\n",
    "\n",
    "            loss = criterion(output, labels)\n",
    "\n",
    "            if phase == 'Train':\n",
    "\n",
    "                #zero gradients\n",
    "                optimizer.zero_grad() \n",
    "\n",
    "                # Backward pass  (calculates the gradients)\n",
    "                loss.backward()   \n",
    "\n",
    "                optimizer.step()             # Updates the weights\n",
    "                \n",
    "                scheduler.step()\n",
    "                \n",
    "                \n",
    "            batch_loss += loss.item()\n",
    "                \n",
    "            _, preds = output.data.max(1)\n",
    "            y_pred.extend(preds.tolist())\n",
    "            y_true.extend(labels.tolist())\n",
    "            \n",
    "            batch_acc = get_accuracy(y_pred, y_true)\n",
    "            \n",
    "            tepoch.set_postfix(loss = batch_loss/(idx+1), accuracy = batch_acc )\n",
    "        \n",
    "\n",
    "\n",
    "          print(confusion_matrix(y_true, y_pred))\n",
    "          pre = precision_score(y_true, y_pred, average='weighted')\n",
    "          recall = recall_score(y_true, y_pred, average='weighted')\n",
    "          f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "        \n",
    "          print(\"F1: {:.4f}, Precision: {:.4f}, Recall : {:.4f}.\".format(f1, pre, recall))\n",
    "        \n",
    "        \n",
    "                    \n",
    "          if phase == 'Val':\n",
    "            if f1 > best_valid_f1:\n",
    "                best_valid_f1 = f1\n",
    "                torch.save(model.state_dict(), PATH)\n",
    "                print('Model Saved!')\n",
    "        \n",
    "          print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a12c060-912a-470a-8f99-abc6ff3e48f9",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "707f8527-9a7c-4db9-91a8-8bbf2df141d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('prompt+adapter_imdb.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a6ca595e-ab6c-4ffa-a596-89907e923c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 312/312 [00:53<00:00,  5.79batch/s, accuracy=0.919, loss=0.231]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "F1: 0.919271, Precision: 0.919290, Recall : 0.919278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "batch_loss = 0.0   #batch loss\n",
    "batch_acc = 0.0   #batch accuracy\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "# set the model to evaluation mode            \n",
    "model.eval()\n",
    "\n",
    "phase = 'Test'\n",
    "\n",
    "with tqdm(test_loader, unit=\"batch\", desc=phase) as tepoch:\n",
    "    \n",
    "    for idx, (data, labels) in enumerate(tepoch):\n",
    "        \n",
    "        input_ids =  data['input_ids'].squeeze(1).to(device)\n",
    "        attention_mask = data['attention_mask'].squeeze(1).to(device)\n",
    "\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "\n",
    "            output = model(input_ids = input_ids, attention_mask = attention_mask)\n",
    "\n",
    "            loss = criterion(output, labels)\n",
    "            \n",
    "            _, preds = output.data.max(1)\n",
    "            y_pred.extend(preds.tolist())\n",
    "            y_true.extend(labels.tolist())\n",
    "            \n",
    "            batch_acc = get_accuracy(y_pred, y_true)\n",
    "            batch_loss += loss.item()\n",
    "            \n",
    "        tepoch.set_postfix(loss = batch_loss/(idx+1), accuracy = batch_acc )\n",
    "\n",
    "\n",
    "pre = precision_score(y_true, y_pred, average='macro')\n",
    "recall = recall_score(y_true, y_pred, average='macro')\n",
    "f1 = f1_score(y_true, y_pred, average='macro')\n",
    "print(\"\")\n",
    "\n",
    "print(\"F1: {:.6f}, Precision: {:.6f}, Recall : {:.6f}\".format(f1, pre, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c60d9e-5fce-441b-b0f2-7eb76bfcbc31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
